# config
#   |- training
#        |- default.yaml
# Default configuration for training

datamodule:
  batch_size: 1
  num_workers: 0

callbacks:
  monitor: "train_loss"
  save_top_k: 3
  mode: "min"

trainer:
  epochs: 100
  accelerator: "gpu"
  devices: 1
  log_every_n_steps: 1
  grad_clip_norm: 1.0
  precision: 16

learning_rate: 1e-4
weight_decay: 1e-5

#------- defined in Glonet class --------

# optimizer:
#   _target_: torch.optim.Adam
#   lr: ${training.learning_rate}
#   weight_decay: ${training.weight_decay}
#   betas: [0.9, 0.999]

# scheduler:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#   T_max: ${training.epochs}
#   eta_min: 1e-6

# loss:
#   _target_: torch.nn.MSELoss
