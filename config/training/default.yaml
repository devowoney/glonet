# config
#   |- training
#        |- default.yaml
# Default configuration for training

datamodule:
  batch_size: 1
  num_workers: 0

callbacks:
  monitor: "train_loss"
  dirpath: ${hydra:run.dir}/checkpoints
  filename: "best-model-{epoch:02d}-{train_loss:.2f}"
  save_top_k: 3
  mode: "min"

tensorboard:
  save_dir: ${hydra:run.dir}
  name: input_${state_number}_optimization
  version: sample_${sample_index}

trainer:
  epochs: 1000
  accelerator: "gpu"
  devices: 1
  log_every_n_steps: 1
  grad_clip_norm: 1.0
  precision: 32

#------- defined in Glonet class --------

optimizer:
  _target_: torch.optim.Adam
  lr: 1e-1
  weight_decay: 0
  betas: [0.9, 0.999]

scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${training.trainer.epochs}
  eta_min: 1e-6

# loss:
#   _target_: torch.nn.MSELoss
