{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "404a08f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing huggingface_hub.hf_api: No module named 'filelock'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.insert(0, \"./src/glonet\")\n",
    "from modelp2 import Glonet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fef1bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n",
      "Device name: NVIDIA H100 NVL\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f9f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Odyssey/public/glonet/TrainedWeights/glonet_p1.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fefb4abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(model_path, map_location=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50229a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device, freeze all parameters, and prepare a gradient-tracked input\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "# Freeze parameters so only input can receive gradients\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.eval()\n",
    "\n",
    "# Try to inspect forward signature to guide a dummy input (best-effort).\n",
    "schema = None\n",
    "try:\n",
    "    if hasattr(model, 'forward'):\n",
    "        try:\n",
    "            schema = str(model.forward.schema)\n",
    "        except Exception:\n",
    "            schema = None\n",
    "except Exception:\n",
    "    schema = None\n",
    "print('forward schema:', schema)\n",
    "\n",
    "# Fallback dummy input: adjust shape if your model expects different dims\n",
    "# Common shape: (batch, channels, H, W). Replace as needed before running.\n",
    "x = torch.randn(1, 2, 5, 672, 1440, device=device, requires_grad=True)\n",
    "\n",
    "# Run forward; models may return tensor, tuple, or dict depending on implementation\n",
    "try:\n",
    "    out = model(x)\n",
    "    print('forward executed')\n",
    "except Exception as e:\n",
    "    print('forward failed:', e)\n",
    "    out = None\n",
    "\n",
    "# Inspect output and attempt a backward pass to verify gradient flows into input\n",
    "if out is None:\n",
    "    print('No output to backprop')\n",
    "else:\n",
    "    # pick a tensor to reduce to scalar for backward\n",
    "    if torch.is_tensor(out):\n",
    "        loss = out.sum()\n",
    "    elif isinstance(out, (tuple, list)):\n",
    "        out_t = None\n",
    "        for o in out:\n",
    "            if torch.is_tensor(o):\n",
    "                out_t = o\n",
    "                break\n",
    "        if out_t is None:\n",
    "            print('no tensor in output to backprop')\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = out_t.sum()\n",
    "    elif isinstance(out, dict):\n",
    "        # pick the first tensor value in dict\n",
    "        loss = None\n",
    "        for v in out.values():\n",
    "            if torch.is_tensor(v):\n",
    "                loss = v.sum()\n",
    "                break\n",
    "    else:\n",
    "        loss = None\n",
    "\n",
    "    if loss is not None:\n",
    "        loss.backward()\n",
    "        if x.grad is not None:\n",
    "            print('input.grad norm:', x.grad.norm().item())\n",
    "        else:\n",
    "            print('input.grad is None — gradient did not flow into input')\n",
    "    else:\n",
    "        print('Could not construct a scalar loss for backward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c903d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac9ddc",
   "metadata": {},
   "source": [
    "**Why the error happens** (short)\n",
    "\n",
    "- That RuntimeError occurs because you called backward() on a tensor that does not require gradients and has no grad_fn. In other words the tensor is not connected to any autograd graph.\n",
    "- Common reasons in your setup:\n",
    " - The object you call backward() on is a Python float or a tensor with requires_grad=False (e.g., you summed or converted to .item(), .numpy(), or detached it).\n",
    " - The model's forward executed inside a torch.no_grad() block or used .detach() / .cpu().numpy() somewhere, preventing gradients.\n",
    " - The TorchScript module you loaded is an inference-only graph that returns detached results (or its forward uses no_grad()).\n",
    " - You froze parameters (that’s fine for input grads) but the forward still prevents grad flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic helper: finds first tensor in output and prints grad info\n",
    "def find_tensor(o):\n",
    "    if torch.is_tensor(o):\n",
    "        return o\n",
    "    if isinstance(o, (list, tuple)):\n",
    "        for v in o:\n",
    "            t = find_tensor(v)\n",
    "            if t is not None:\n",
    "                return t\n",
    "    if isinstance(o, dict):\n",
    "        for v in o.values():\n",
    "            t = find_tensor(v)\n",
    "            if t is not None:\n",
    "                return t\n",
    "    return None\n",
    "\n",
    "out_t = find_tensor(out)\n",
    "print(\"x.requires_grad:\", getattr(x, \"requires_grad\", None))\n",
    "if out_t is None:\n",
    "    print(\"No tensor found in model output (out is None or not tensor-like).\")\n",
    "else:\n",
    "    print(\"out_t type:\", type(out_t))\n",
    "    print(\"out_t.requires_grad:\", out_t.requires_grad)\n",
    "    print(\"out_t.grad_fn:\", out_t.grad_fn)\n",
    "    # If it does not require grad, show likely suspects\n",
    "    if not out_t.requires_grad:\n",
    "        print(\"-> Output does not require grad. Check for torch.no_grad(), .detach(), or conversions to numpy/float inside forward.\")\n",
    "    else:\n",
    "        # safe backward check\n",
    "        loss = out_t.sum()\n",
    "        print(\"loss.requires_grad:\", loss.requires_grad)\n",
    "        loss.backward()\n",
    "        print(\"x.grad is None?:\", x.grad is None)\n",
    "        if x.grad is not None:\n",
    "            print(\"x.grad norm:\", x.grad.norm().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef224ac",
   "metadata": {},
   "source": [
    "***How to interpret results and fixes***\n",
    "\n",
    "- If `out_t.requires_grad` is False:\n",
    "  - Inspect your model forward for torch.no_grad(), .detach(), or explicit .cpu().numpy() / .item() conversions. Remove or alter them so the forward keeps tensors connected to the graph.\n",
    "  - If you loaded a scripted/inference-only TorchScript (from tracing or an exported inference build), load the original model class + state_dict instead (instantiate `Glonet(...)`, load state_dict) so autograd is available.\n",
    "- If the output is a Python float or you see `loss` created from `.item()` / `.numpy()`, don't convert to float before backward; keep it as a tensor.\n",
    "- If `x.requires_grad` is False (shouldn’t be in your code): recreate x with requires_grad=True.\n",
    "- If model intentionally uses `with torch.no_grad()` for some ops, remove that block for the ops that must be differentiable, or implement a separate \"differentiable forward\" for optimization.\n",
    "\n",
    "If you want, I can:\n",
    "\n",
    "- Edit the notebook to replace the fallback dummy input shape with the correct shape automatically (I can try to infer it), and add the diagnostic prints directly into the cell.\n",
    "- Help load the model from source/state_dict instead of using `torch.jit.load` (if you have the class definition and checkpoint).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb84f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n"
     ]
    }
   ],
   "source": [
    "new_model = Glonet(shape_in=(2, 5, 672, 1440))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fbb09aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Glonet(\n",
       "  (jump): residual(\n",
       "    (maps): Encoder(\n",
       "      (enc): Sequential(\n",
       "        (0): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): Conv2d(5, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (2): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mmb): MMB(\n",
       "      (enc): Sequential(\n",
       "        (0): Inception(\n",
       "          (conv1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (dec): Sequential(\n",
       "        (0): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Inception(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Inception(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Inception(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Inception(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Inception(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): Inception(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Inception(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mapsback): Decoder(\n",
       "      (dec): Sequential(\n",
       "        (0): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (2): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (readout): Conv2d(16, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (space): FoTF(\n",
       "    (NN): Local_CNN_Branch(\n",
       "      (upconv): ConvTranspose2d(5, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (NO): GF_Block(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (projection): Conv2d(5, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blks): ModuleList(\n",
       "        (0-11): 12 x FourierNetBlock(\n",
       "          (normlayer1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (filter): AdativeFourierNeuralOperator(\n",
       "            (relu): ReLU()\n",
       "            (bias): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (attn): MultiHeadSelfAttention(\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (normlayer2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (fc3): AdaptiveAvgPool1d(output_size=768)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (lproj): Sequential(\n",
       "        (transposeconv1): ConvTranspose2d(768, 80, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (act1): Tanh()\n",
       "        (transposeconv2): ConvTranspose2d(80, 20, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (act2): Tanh()\n",
       "        (transposeconv3): ConvTranspose2d(20, 5, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "      (final_dropout): Identity()\n",
       "    )\n",
       "    (up): ConvTranspose2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv1x1): Conv2d(5, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (maps): Encoder(\n",
       "    (enc): Sequential(\n",
       "      (0): ConvSC(\n",
       "        (conv): BasicConv2d(\n",
       "          (conv): Conv2d(5, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(2, 256, eps=1e-05, affine=True)\n",
       "          (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ConvSC(\n",
       "        (conv): BasicConv2d(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (norm): GroupNorm(2, 256, eps=1e-05, affine=True)\n",
       "          (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dynamics): TeDev(\n",
       "    (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "    (enc): Sequential(\n",
       "      (0): Inception(\n",
       "        (conv1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x FourierNetBlock(\n",
       "        (normlayer1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (filter): AdativeFourierNeuralOperator(\n",
       "          (relu): ReLU()\n",
       "          (bias): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (normlayer2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (fc3): AdaptiveAvgPool1d(output_size=128)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dec): Sequential(\n",
       "      (0): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Inception(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Inception(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Inception(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): Inception(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Inception(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): Inception(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Inception(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 512, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mapsback): Decoder(\n",
       "    (dec): Sequential(\n",
       "      (0): ConvSC(\n",
       "        (conv): BasicConv2d(\n",
       "          (conv): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (norm): GroupNorm(2, 256, eps=1e-05, affine=True)\n",
       "          (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ConvSC(\n",
       "        (conv): BasicConv2d(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(2, 256, eps=1e-05, affine=True)\n",
       "          (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): Conv2d(256, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "712290e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=Sequential\n",
       "  (0): RecursiveScriptModule(\n",
       "    original_name=Glonet\n",
       "    (jump): RecursiveScriptModule(\n",
       "      original_name=residual\n",
       "      (maps): RecursiveScriptModule(\n",
       "        original_name=Encoder\n",
       "        (enc): RecursiveScriptModule(\n",
       "          original_name=Sequential\n",
       "          (0): RecursiveScriptModule(\n",
       "            original_name=ConvSC\n",
       "            (conv): RecursiveScriptModule(\n",
       "              original_name=BasicConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (act): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "          (1): RecursiveScriptModule(\n",
       "            original_name=ConvSC\n",
       "            (conv): RecursiveScriptModule(\n",
       "              original_name=BasicConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (act): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "          (2): RecursiveScriptModule(\n",
       "            original_name=ConvSC\n",
       "            (conv): RecursiveScriptModule(\n",
       "              original_name=BasicConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (act): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "          (3): RecursiveScriptModule(\n",
       "            original_name=ConvSC\n",
       "            (conv): RecursiveScriptModule(\n",
       "              original_name=BasicConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (act): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mmb): RecursiveScriptModule(\n",
       "        original_name=MMB\n",
       "        (enc): RecursiveScriptModule(\n",
       "          original_name=Sequential\n",
       "          (0): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (7): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dec): RecursiveScriptModule(\n",
       "          original_name=Sequential\n",
       "          (0): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (7): RecursiveScriptModule(\n",
       "            original_name=Inception\n",
       "            (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (lls): RecursiveScriptModule(\n",
       "              original_name=Sequential\n",
       "              (0): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (1): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (2): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "              (3): RecursiveScriptModule(\n",
       "                original_name=GroupConv2d\n",
       "                (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "                (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "                (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mapsback): RecursiveScriptModule(\n",
       "        original_name=Decoder\n",
       "        (dec): RecursiveScriptModule(\n",
       "          original_name=Sequential\n",
       "          (0): RecursiveScriptModule(\n",
       "            original_name=ConvSC\n",
       "            (conv): RecursiveScriptModule(\n",
       "              original_name=BasicConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=ConvTranspose2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (act): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "          (1): RecursiveScriptModule(\n",
       "            original_name=ConvSC\n",
       "            (conv): RecursiveScriptModule(\n",
       "              original_name=BasicConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (act): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "          (2): RecursiveScriptModule(\n",
       "            original_name=ConvSC\n",
       "            (conv): RecursiveScriptModule(\n",
       "              original_name=BasicConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=ConvTranspose2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (act): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "          (3): RecursiveScriptModule(\n",
       "            original_name=ConvSC\n",
       "            (conv): RecursiveScriptModule(\n",
       "              original_name=BasicConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (act): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (readout): RecursiveScriptModule(original_name=Conv2d)\n",
       "      )\n",
       "    )\n",
       "    (space): RecursiveScriptModule(\n",
       "      original_name=FoTF\n",
       "      (NN): RecursiveScriptModule(\n",
       "        original_name=Local_CNN_Branch\n",
       "        (upconv): RecursiveScriptModule(original_name=ConvTranspose2d)\n",
       "      )\n",
       "      (NO): RecursiveScriptModule(\n",
       "        original_name=GF_Block\n",
       "        (patch_embed): RecursiveScriptModule(\n",
       "          original_name=PatchEmbed\n",
       "          (projection): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (norm): RecursiveScriptModule(original_name=Identity)\n",
       "        )\n",
       "        (pos_drop): RecursiveScriptModule(original_name=Dropout)\n",
       "        (blks): RecursiveScriptModule(\n",
       "          original_name=ModuleList\n",
       "          (0): RecursiveScriptModule(\n",
       "            original_name=FourierNetBlock\n",
       "            (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (filter): RecursiveScriptModule(\n",
       "              original_name=AdativeFourierNeuralOperator\n",
       "              (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "              (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "            )\n",
       "            (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "            (attn): RecursiveScriptModule(\n",
       "              original_name=MultiHeadSelfAttention\n",
       "              (multihead_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "              )\n",
       "            )\n",
       "            (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (mlp): RecursiveScriptModule(\n",
       "              original_name=Mlp\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (act): RecursiveScriptModule(original_name=GELU)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "              (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (1): RecursiveScriptModule(\n",
       "            original_name=FourierNetBlock\n",
       "            (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (filter): RecursiveScriptModule(\n",
       "              original_name=AdativeFourierNeuralOperator\n",
       "              (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "              (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "            )\n",
       "            (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "            (attn): RecursiveScriptModule(\n",
       "              original_name=MultiHeadSelfAttention\n",
       "              (multihead_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "              )\n",
       "            )\n",
       "            (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (mlp): RecursiveScriptModule(\n",
       "              original_name=Mlp\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (act): RecursiveScriptModule(original_name=GELU)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "              (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (2): RecursiveScriptModule(\n",
       "            original_name=FourierNetBlock\n",
       "            (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (filter): RecursiveScriptModule(\n",
       "              original_name=AdativeFourierNeuralOperator\n",
       "              (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "              (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "            )\n",
       "            (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "            (attn): RecursiveScriptModule(\n",
       "              original_name=MultiHeadSelfAttention\n",
       "              (multihead_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "              )\n",
       "            )\n",
       "            (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (mlp): RecursiveScriptModule(\n",
       "              original_name=Mlp\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (act): RecursiveScriptModule(original_name=GELU)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "              (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (3): RecursiveScriptModule(\n",
       "            original_name=FourierNetBlock\n",
       "            (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (filter): RecursiveScriptModule(\n",
       "              original_name=AdativeFourierNeuralOperator\n",
       "              (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "              (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "            )\n",
       "            (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "            (attn): RecursiveScriptModule(\n",
       "              original_name=MultiHeadSelfAttention\n",
       "              (multihead_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "              )\n",
       "            )\n",
       "            (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (mlp): RecursiveScriptModule(\n",
       "              original_name=Mlp\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (act): RecursiveScriptModule(original_name=GELU)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "              (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (4): RecursiveScriptModule(\n",
       "            original_name=FourierNetBlock\n",
       "            (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (filter): RecursiveScriptModule(\n",
       "              original_name=AdativeFourierNeuralOperator\n",
       "              (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "              (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "            )\n",
       "            (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "            (attn): RecursiveScriptModule(\n",
       "              original_name=MultiHeadSelfAttention\n",
       "              (multihead_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "              )\n",
       "            )\n",
       "            (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (mlp): RecursiveScriptModule(\n",
       "              original_name=Mlp\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (act): RecursiveScriptModule(original_name=GELU)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "              (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (5): RecursiveScriptModule(\n",
       "            original_name=FourierNetBlock\n",
       "            (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (filter): RecursiveScriptModule(\n",
       "              original_name=AdativeFourierNeuralOperator\n",
       "              (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "              (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "            )\n",
       "            (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "            (attn): RecursiveScriptModule(\n",
       "              original_name=MultiHeadSelfAttention\n",
       "              (multihead_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "              )\n",
       "            )\n",
       "            (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (mlp): RecursiveScriptModule(\n",
       "              original_name=Mlp\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (act): RecursiveScriptModule(original_name=GELU)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "              (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (6): RecursiveScriptModule(\n",
       "            original_name=FourierNetBlock\n",
       "            (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (filter): RecursiveScriptModule(\n",
       "              original_name=AdativeFourierNeuralOperator\n",
       "              (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "              (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "            )\n",
       "            (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "            (attn): RecursiveScriptModule(\n",
       "              original_name=MultiHeadSelfAttention\n",
       "              (multihead_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "              )\n",
       "            )\n",
       "            (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (mlp): RecursiveScriptModule(\n",
       "              original_name=Mlp\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (act): RecursiveScriptModule(original_name=GELU)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "              (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (7): RecursiveScriptModule(\n",
       "            original_name=FourierNetBlock\n",
       "            (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (filter): RecursiveScriptModule(\n",
       "              original_name=AdativeFourierNeuralOperator\n",
       "              (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "              (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "            )\n",
       "            (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "            (attn): RecursiveScriptModule(\n",
       "              original_name=MultiHeadSelfAttention\n",
       "              (multihead_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "              )\n",
       "            )\n",
       "            (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (mlp): RecursiveScriptModule(\n",
       "              original_name=Mlp\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (act): RecursiveScriptModule(original_name=GELU)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "              (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (8): RecursiveScriptModule(\n",
       "            original_name=FourierNetBlock\n",
       "            (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (filter): RecursiveScriptModule(\n",
       "              original_name=AdativeFourierNeuralOperator\n",
       "              (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "              (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "            )\n",
       "            (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "            (attn): RecursiveScriptModule(\n",
       "              original_name=MultiHeadSelfAttention\n",
       "              (multihead_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "              )\n",
       "            )\n",
       "            (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (mlp): RecursiveScriptModule(\n",
       "              original_name=Mlp\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (act): RecursiveScriptModule(original_name=GELU)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "              (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (9): RecursiveScriptModule(\n",
       "            original_name=FourierNetBlock\n",
       "            (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (filter): RecursiveScriptModule(\n",
       "              original_name=AdativeFourierNeuralOperator\n",
       "              (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "              (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "            )\n",
       "            (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "            (attn): RecursiveScriptModule(\n",
       "              original_name=MultiHeadSelfAttention\n",
       "              (multihead_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "              )\n",
       "            )\n",
       "            (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (mlp): RecursiveScriptModule(\n",
       "              original_name=Mlp\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (act): RecursiveScriptModule(original_name=GELU)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "              (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (10): RecursiveScriptModule(\n",
       "            original_name=FourierNetBlock\n",
       "            (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (filter): RecursiveScriptModule(\n",
       "              original_name=AdativeFourierNeuralOperator\n",
       "              (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "              (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "            )\n",
       "            (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "            (attn): RecursiveScriptModule(\n",
       "              original_name=MultiHeadSelfAttention\n",
       "              (multihead_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "              )\n",
       "            )\n",
       "            (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (mlp): RecursiveScriptModule(\n",
       "              original_name=Mlp\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (act): RecursiveScriptModule(original_name=GELU)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "              (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (11): RecursiveScriptModule(\n",
       "            original_name=FourierNetBlock\n",
       "            (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (filter): RecursiveScriptModule(\n",
       "              original_name=AdativeFourierNeuralOperator\n",
       "              (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "              (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "            )\n",
       "            (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "            (attn): RecursiveScriptModule(\n",
       "              original_name=MultiHeadSelfAttention\n",
       "              (multihead_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "              )\n",
       "            )\n",
       "            (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (mlp): RecursiveScriptModule(\n",
       "              original_name=Mlp\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (act): RecursiveScriptModule(original_name=GELU)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "              (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "        (lproj): RecursiveScriptModule(\n",
       "          original_name=Sequential\n",
       "          (transposeconv1): RecursiveScriptModule(original_name=ConvTranspose2d)\n",
       "          (act1): RecursiveScriptModule(original_name=Tanh)\n",
       "          (transposeconv2): RecursiveScriptModule(original_name=ConvTranspose2d)\n",
       "          (act2): RecursiveScriptModule(original_name=Tanh)\n",
       "          (transposeconv3): RecursiveScriptModule(original_name=ConvTranspose2d)\n",
       "        )\n",
       "        (final_dropout): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "      (up): RecursiveScriptModule(original_name=ConvTranspose2d)\n",
       "      (down): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (conv1x1): RecursiveScriptModule(original_name=Conv2d)\n",
       "    )\n",
       "    (maps): RecursiveScriptModule(\n",
       "      original_name=Encoder\n",
       "      (enc): RecursiveScriptModule(\n",
       "        original_name=Sequential\n",
       "        (0): RecursiveScriptModule(\n",
       "          original_name=ConvSC\n",
       "          (conv): RecursiveScriptModule(\n",
       "            original_name=BasicConv2d\n",
       "            (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "            (act): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "          )\n",
       "        )\n",
       "        (1): RecursiveScriptModule(\n",
       "          original_name=ConvSC\n",
       "          (conv): RecursiveScriptModule(\n",
       "            original_name=BasicConv2d\n",
       "            (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "            (act): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dynamics): RecursiveScriptModule(\n",
       "      original_name=TeDev\n",
       "      (norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "      (enc): RecursiveScriptModule(\n",
       "        original_name=Sequential\n",
       "        (0): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (blocks): RecursiveScriptModule(\n",
       "        original_name=ModuleList\n",
       "        (0): RecursiveScriptModule(\n",
       "          original_name=FourierNetBlock\n",
       "          (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (filter): RecursiveScriptModule(\n",
       "            original_name=AdativeFourierNeuralOperator\n",
       "            (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "            (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "          )\n",
       "          (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "          (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (mlp): RecursiveScriptModule(\n",
       "            original_name=Mlp\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (act): RecursiveScriptModule(original_name=GELU)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "            (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (1): RecursiveScriptModule(\n",
       "          original_name=FourierNetBlock\n",
       "          (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (filter): RecursiveScriptModule(\n",
       "            original_name=AdativeFourierNeuralOperator\n",
       "            (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "            (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "          )\n",
       "          (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "          (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (mlp): RecursiveScriptModule(\n",
       "            original_name=Mlp\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (act): RecursiveScriptModule(original_name=GELU)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "            (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (2): RecursiveScriptModule(\n",
       "          original_name=FourierNetBlock\n",
       "          (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (filter): RecursiveScriptModule(\n",
       "            original_name=AdativeFourierNeuralOperator\n",
       "            (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "            (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "          )\n",
       "          (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "          (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (mlp): RecursiveScriptModule(\n",
       "            original_name=Mlp\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (act): RecursiveScriptModule(original_name=GELU)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "            (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (3): RecursiveScriptModule(\n",
       "          original_name=FourierNetBlock\n",
       "          (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (filter): RecursiveScriptModule(\n",
       "            original_name=AdativeFourierNeuralOperator\n",
       "            (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "            (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "          )\n",
       "          (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "          (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (mlp): RecursiveScriptModule(\n",
       "            original_name=Mlp\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (act): RecursiveScriptModule(original_name=GELU)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "            (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (4): RecursiveScriptModule(\n",
       "          original_name=FourierNetBlock\n",
       "          (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (filter): RecursiveScriptModule(\n",
       "            original_name=AdativeFourierNeuralOperator\n",
       "            (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "            (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "          )\n",
       "          (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "          (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (mlp): RecursiveScriptModule(\n",
       "            original_name=Mlp\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (act): RecursiveScriptModule(original_name=GELU)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "            (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (5): RecursiveScriptModule(\n",
       "          original_name=FourierNetBlock\n",
       "          (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (filter): RecursiveScriptModule(\n",
       "            original_name=AdativeFourierNeuralOperator\n",
       "            (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "            (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "          )\n",
       "          (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "          (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (mlp): RecursiveScriptModule(\n",
       "            original_name=Mlp\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (act): RecursiveScriptModule(original_name=GELU)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "            (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (6): RecursiveScriptModule(\n",
       "          original_name=FourierNetBlock\n",
       "          (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (filter): RecursiveScriptModule(\n",
       "            original_name=AdativeFourierNeuralOperator\n",
       "            (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "            (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "          )\n",
       "          (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "          (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (mlp): RecursiveScriptModule(\n",
       "            original_name=Mlp\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (act): RecursiveScriptModule(original_name=GELU)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "            (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (7): RecursiveScriptModule(\n",
       "          original_name=FourierNetBlock\n",
       "          (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (filter): RecursiveScriptModule(\n",
       "            original_name=AdativeFourierNeuralOperator\n",
       "            (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "            (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "          )\n",
       "          (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "          (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (mlp): RecursiveScriptModule(\n",
       "            original_name=Mlp\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (act): RecursiveScriptModule(original_name=GELU)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "            (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (8): RecursiveScriptModule(\n",
       "          original_name=FourierNetBlock\n",
       "          (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (filter): RecursiveScriptModule(\n",
       "            original_name=AdativeFourierNeuralOperator\n",
       "            (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "            (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "          )\n",
       "          (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "          (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (mlp): RecursiveScriptModule(\n",
       "            original_name=Mlp\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (act): RecursiveScriptModule(original_name=GELU)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "            (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (9): RecursiveScriptModule(\n",
       "          original_name=FourierNetBlock\n",
       "          (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (filter): RecursiveScriptModule(\n",
       "            original_name=AdativeFourierNeuralOperator\n",
       "            (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "            (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "          )\n",
       "          (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "          (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (mlp): RecursiveScriptModule(\n",
       "            original_name=Mlp\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (act): RecursiveScriptModule(original_name=GELU)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "            (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (10): RecursiveScriptModule(\n",
       "          original_name=FourierNetBlock\n",
       "          (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (filter): RecursiveScriptModule(\n",
       "            original_name=AdativeFourierNeuralOperator\n",
       "            (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "            (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "          )\n",
       "          (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "          (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (mlp): RecursiveScriptModule(\n",
       "            original_name=Mlp\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (act): RecursiveScriptModule(original_name=GELU)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "            (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (11): RecursiveScriptModule(\n",
       "          original_name=FourierNetBlock\n",
       "          (normlayer1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (filter): RecursiveScriptModule(\n",
       "            original_name=AdativeFourierNeuralOperator\n",
       "            (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "            (bias): RecursiveScriptModule(original_name=Conv1d)\n",
       "          )\n",
       "          (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "          (normlayer2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (mlp): RecursiveScriptModule(\n",
       "            original_name=Mlp\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (act): RecursiveScriptModule(original_name=GELU)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc3): RecursiveScriptModule(original_name=AdaptiveAvgPool1d)\n",
       "            (drop): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (dec): RecursiveScriptModule(\n",
       "        original_name=Sequential\n",
       "        (0): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): RecursiveScriptModule(\n",
       "          original_name=Inception\n",
       "          (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (lls): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=GroupConv2d\n",
       "              (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "              (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "              (activate): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mapsback): RecursiveScriptModule(\n",
       "      original_name=Decoder\n",
       "      (dec): RecursiveScriptModule(\n",
       "        original_name=Sequential\n",
       "        (0): RecursiveScriptModule(\n",
       "          original_name=ConvSC\n",
       "          (conv): RecursiveScriptModule(\n",
       "            original_name=BasicConv2d\n",
       "            (conv): RecursiveScriptModule(original_name=ConvTranspose2d)\n",
       "            (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "            (act): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "          )\n",
       "        )\n",
       "        (1): RecursiveScriptModule(\n",
       "          original_name=ConvSC\n",
       "          (conv): RecursiveScriptModule(\n",
       "            original_name=BasicConv2d\n",
       "            (conv): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (norm): RecursiveScriptModule(original_name=GroupNorm)\n",
       "            (act): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (readout): RecursiveScriptModule(original_name=Conv2d)\n",
       "    )\n",
       "  )\n",
       "  (1): RecursiveScriptModule(original_name=SelectiveFilterLayer)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab633e",
   "metadata": {},
   "source": [
    "Inspect `model` contents — diagnostic snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de291c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnostic\n",
    "print(\"type(model):\", type(model))\n",
    "print(\"hasattr(model, 'state_dict'):\", hasattr(model, 'state_dict'))\n",
    "print(\"hasattr(model, 'named_parameters'):\", hasattr(model, 'named_parameters'))\n",
    "try:\n",
    "    sd = model.state_dict()\n",
    "    print(\"state_dict keys sample:\", list(sd.keys())[:20])\n",
    "    print(\"state_dict len:\", len(sd))\n",
    "except Exception as e:\n",
    "    print(\"state_dict() not usable:\", e)\n",
    "\n",
    "# also inspect a sample of named parameters/buffers\n",
    "if hasattr(model, 'named_parameters'):\n",
    "    print(\"sample named_parameters:\", [n for n, _ in list(model.named_parameters())[:20]])\n",
    "if hasattr(model, 'named_buffers'):\n",
    "    print(\"sample named_buffers:\", [n for n, _ in list(model.named_buffers())[:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b225c",
   "metadata": {},
   "source": [
    "If `state_dict` available: `use new_model.load_state_dict(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try re-loading file as a dict first (safe, no overwrite of 'model' variable)\n",
    "ck = torch.jit.load(model_path, map_location=torch.device('cuda'))\n",
    "if isinstance(ck, dict):\n",
    "    # common wrappers\n",
    "    if 'model_state_dict' in ck:\n",
    "        sd = ck['model_state_dict']\n",
    "    elif 'state_dict' in ck:\n",
    "        sd = ck['state_dict']\n",
    "    else:\n",
    "        sd = ck\n",
    "    # Attempt to load\n",
    "    missing, unexpected = new_model.load_state_dict(sd, strict=False)\n",
    "    print(\"missing keys:\", missing)\n",
    "    print(\"unexpected keys:\", unexpected)\n",
    "else:\n",
    "    print(\"torch.load returned non-dict (it's probably a ScriptModule).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6655d94",
   "metadata": {},
   "source": [
    "If `model.state_dict()` is available (ScriptModule may support this), try direct load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sd = model.state_dict()\n",
    "    missing, unexpected = new_model.load_state_dict(sd, strict=False)\n",
    "    print(\"loaded state_dict -> missing:\", missing, \" unexpected:\", unexpected)\n",
    "except Exception as e:\n",
    "    print(\"model.state_dict() not available:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5a72a",
   "metadata": {},
   "source": [
    "Otherwise: copy named_parameters and named_buffers, or fall back to shape-based mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "new_model.to(device)\n",
    "\n",
    "# collect source params/buffers\n",
    "src_params = {n: p for n, p in model.named_parameters()} if hasattr(model, 'named_parameters') else {}\n",
    "src_bufs   = {n: b for n, b in model.named_buffers()}    if hasattr(model, 'named_buffers')    else {}\n",
    "\n",
    "copied = []\n",
    "with torch.no_grad():\n",
    "    for name, tgt in new_model.named_parameters():\n",
    "        src = src_params.get(name)\n",
    "        if src is None:\n",
    "            continue\n",
    "        if tuple(src.shape) == tuple(tgt.shape):\n",
    "            tgt.copy_(src.to(tgt.device).to(tgt.dtype))\n",
    "            copied.append(name)\n",
    "\n",
    "    for name, tgt in new_model.named_buffers():\n",
    "        src = src_bufs.get(name)\n",
    "        if src is None:\n",
    "            continue\n",
    "        if tuple(src.shape) == tuple(tgt.shape):\n",
    "            tgt.copy_(src.to(tgt.device).to(tgt.dtype))\n",
    "\n",
    "print(\"params copied by name:\", len(copied))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5af0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build lists of remaining src/tgt tensors\n",
    "remaining_src = [p for n, p in src_params.items() if n not in set(copied)]\n",
    "remaining_tgt = [(n, p) for n, p in new_model.named_parameters() if n not in set(copied)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for tgt_name, tgt in remaining_tgt:\n",
    "        for i, src in enumerate(remaining_src):\n",
    "            if tuple(src.shape) == tuple(tgt.shape):\n",
    "                tgt.copy_(src.to(tgt.device).to(tgt.dtype))\n",
    "                print(f\"copied by shape: {tgt_name} <- src_index_{i} shape={src.shape}\")\n",
    "                remaining_src.pop(i)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare parameters and buffers\n",
    "def compare_parameters(source_models, merged_model):\n",
    "    for i, source_model in enumerate(source_models):\n",
    "        print(f\"Comparing parameters for source model {i + 1}...\")\n",
    "        src_params = {n: p for n, p in source_model.named_parameters()}\n",
    "        src_bufs = {n: b for n, b in source_model.named_buffers()}\n",
    "        \n",
    "        merged_params = {n: p for n, p in merged_model.named_parameters()}\n",
    "        merged_bufs = {n: b for n, b in merged_model.named_buffers()}\n",
    "        \n",
    "        # Check parameters\n",
    "        for name, param in src_params.items():\n",
    "            if name in merged_params:\n",
    "                if param.shape != merged_params[name].shape:\n",
    "                    print(f\"Shape mismatch for parameter '{name}': {param.shape} vs {merged_params[name].shape}\")\n",
    "                elif not torch.allclose(param, merged_params[name], atol=1e-6):\n",
    "                    print(f\"Value mismatch for parameter '{name}'\")\n",
    "            else:\n",
    "                print(f\"Parameter '{name}' not found in merged model.\")\n",
    "        \n",
    "        # Check buffers\n",
    "        for name, buf in src_bufs.items():\n",
    "            if name in merged_bufs:\n",
    "                if buf.shape != merged_bufs[name].shape:\n",
    "                    print(f\"Shape mismatch for buffer '{name}': {buf.shape} vs {merged_bufs[name].shape}\")\n",
    "                elif not torch.allclose(buf, merged_bufs[name], atol=1e-6):\n",
    "                    print(f\"Value mismatch for buffer '{name}'\")\n",
    "            else:\n",
    "                print(f\"Buffer '{name}' not found in merged model.\")\n",
    "        print(f\"Finished comparing source model {i + 1}.\\n\")\n",
    "\n",
    "# Example usage\n",
    "source_models = [\n",
    "    torch.jit.load(\"/Odyssey/public/glonet/TrainedWeights/glonet_p1.pt\", map_location='cpu'),\n",
    "    torch.jit.load(\"/Odyssey/public/glonet/TrainedWeights/glonet_p2.pt\", map_location='cpu'),\n",
    "    torch.jit.load(\"/Odyssey/public/glonet/TrainedWeights/glonet_p3.pt\", map_location='cpu')\n",
    "]\n",
    "\n",
    "# Ensure the merged model is created before running this\n",
    "compare_parameters(source_models, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a22deb",
   "metadata": {},
   "source": [
    "- If `model` is a traced/inference TorchScript that explicitly uses `torch.no_grad()` or detaches its outputs, gradients cannot flow even if you copy weights; you'll need the original model class + state_dict to get a differentiable forward.\n",
    "- Name mismatches are common when model code changed or when using wrappers (DataParallel, prefix differences). Manual mapping may be required.\n",
    "- Always check shapes before copying; copying mismatched shapes will raise errors.\n",
    "- After copying, set `new_model.eval()` or `.train()` as needed, and move it to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.eval()\n",
    "# create a small dummy input matching new_model expectation and run a forward\n",
    "x_test = torch.randn(1, 2, 5, 672, 1440, device=device)  # replace with correct dims\n",
    "try:\n",
    "    out = new_model(x_test)\n",
    "    print(\"new_model forward OK, out type:\", type(out))\n",
    "except Exception as e:\n",
    "    print(\"forward failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df41862",
   "metadata": {},
   "source": [
    "- ScriptModule often supports `state_dict()` and `named_parameters()`; prefer `state_dict()` when available.\n",
    "- Always use `torch.no_grad()` when copying `.data` to avoid creating graph connections.\n",
    "- Ensure dtype/device match: use `.to(tgt.device).to(tgt.dtype)` when copying.\n",
    "- If names differ due to wrappers or refactoring, manual name mapping might be needed.\n",
    "- If the ScriptModule was traced/inference-only, weights still copy fine; the problem you had earlier came from trying `torch.load` with the wrong flags — using `torch.jit.load` (what you already did) is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa7222",
   "metadata": {},
   "source": [
    "Empty gpu's memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e7c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "model.to('cpu')\n",
    "del x_test\n",
    "del out\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "try:\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test that new_model allows gradients w.r.t. input (freeze weights, track input)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "new_model.to(device)\n",
    "# Freeze model weights\n",
    "for p in new_model.parameters():\n",
    "    p.requires_grad = False\n",
    "new_model.eval()\n",
    "\n",
    "# Create input with requires_grad=True. Adjust shape if your model expects different dims.\n",
    "x = torch.randn(1, 2, 5, 672, 1440, device=device, requires_grad=True)\n",
    "print('x.requires_grad:', x.requires_grad)\n",
    "\n",
    "# Forward pass and inspect output's grad properties\n",
    "try:\n",
    "    out = new_model(x)\n",
    "    print('forward ok, out type:', type(out))\n",
    "except Exception as e:\n",
    "    print('forward failed:', e)\n",
    "    out = None\n",
    "\n",
    "def first_tensor(o):\n",
    "    if torch.is_tensor(o):\n",
    "        return o\n",
    "    if isinstance(o, (list, tuple)):\n",
    "        for v in o:\n",
    "            t = first_tensor(v)\n",
    "            if t is not None:\n",
    "                return t\n",
    "    if isinstance(o, dict):\n",
    "        for v in o.values():\n",
    "            t = first_tensor(v)\n",
    "            if t is not None:\n",
    "                return t\n",
    "    return None\n",
    "\n",
    "if out is None:\n",
    "    print('No output to test')\n",
    "else:\n",
    "    out_t = first_tensor(out)\n",
    "    if out_t is None:\n",
    "        print('No tensor found inside model output')\n",
    "    else:\n",
    "        print('out_t.shape:', getattr(out_t, 'shape', None))\n",
    "        print('out_t.requires_grad:', out_t.requires_grad)\n",
    "        print('out_t.grad_fn:', out_t.grad_fn)\n",
    "        if not out_t.requires_grad:\n",
    "            print('Output does not require grad — likely the forward detached tensors or used no_grad.')\n",
    "        else:\n",
    "            # backprop and check x.grad\n",
    "            loss = out_t.sum()\n",
    "            print('loss.requires_grad:', loss.requires_grad)\n",
    "            loss.backward()\n",
    "            if x.grad is None:\n",
    "                print('x.grad is None — gradient did not flow into input')\n",
    "            else:\n",
    "                print('x.grad norm:', x.grad.norm().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbf5968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple gradient descent on input `x` to verify gradients flow into the input\n",
    "# If `x` exists from earlier cells we reuse it, otherwise create a random init.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "new_model.to(device)\n",
    "for p in new_model.parameters():\n",
    "    p.requires_grad = False\n",
    "new_model.eval()\n",
    "\n",
    "try:\n",
    "    x_init = x.detach().clone().to(device)\n",
    "    print('reusing existing `x` from earlier cells')\n",
    "except NameError:\n",
    "    x_init = torch.randn(1, 2, 5, 672, 1440, device=device)\n",
    "    print('created new random `x_init`')\n",
    "\n",
    "# Wrap the input as a Parameter so it can be optimized by an optimizer\n",
    "x_param = torch.nn.Parameter(x_init, requires_grad=True)\n",
    "opt = torch.optim.SGD([x_param], lr=1e-2, momentum=0.7)\n",
    "\n",
    "start_norm = x_param.data.norm().item()\n",
    "print(f'start x norm: {start_norm:.6f}')\n",
    "\n",
    "# small training loop\n",
    "steps = 100\n",
    "for i in range(steps):\n",
    "    opt.zero_grad()\n",
    "    out = new_model(x_param)\n",
    "\n",
    "    # helper to find first tensor in output\n",
    "    def first_tensor(o):\n",
    "        if torch.is_tensor(o):\n",
    "            return o\n",
    "        if isinstance(o, (list, tuple)):\n",
    "            for v in o:\n",
    "                t = first_tensor(v)\n",
    "                if t is not None:\n",
    "                    return t\n",
    "        if isinstance(o, dict):\n",
    "            for v in o.values():\n",
    "                t = first_tensor(v)\n",
    "                if t is not None:\n",
    "                    return t\n",
    "        return None\n",
    "\n",
    "    out_t = first_tensor(out)\n",
    "    if out_t is None:\n",
    "        print('Model returned no tensor; cannot compute loss. Stopping.')\n",
    "        break\n",
    "\n",
    "    loss = out_t.sum()  # simple scalar to exercise gradients\n",
    "    loss.backward()\n",
    "\n",
    "    grad_norm = x_param.grad.norm().item() if x_param.grad is not None else float('nan')\n",
    "    print(f'step {i:02d} loss={loss.item():.6e} x.grad_norm={grad_norm:.6e} x.norm={x_param.data.norm().item():.6f}')\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "end_norm = x_param.data.norm().item()\n",
    "print(f'end x norm: {end_norm:.6f} (changed by {end_norm - start_norm:.6f})')\n",
    "print('final x_param.requires_grad:', x_param.requires_grad)\n",
    "print('out_t.requires_grad:', getattr(out_t, 'requires_grad', None), 'out_t.grad_fn:', getattr(out_t, 'grad_fn', None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3344bf",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e063663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROBLEM DIAGNOSIS ===\n",
      "The model's forward method contains .detach() calls that break gradient flow.\n",
      "From modelp2.py lines 53-54: skip_feature=skip_feature.detach(), spatial_feature=spatial_feature.detach()\n",
      "And more .detach() calls throughout the forward method.\n",
      "\n",
      "=== SOLUTION 1: Create differentiable model from original class ===\n",
      "720\n",
      "✓ Loaded state_dict - missing: 1033, unexpected: 1033\n",
      "✓ Created differentiable model, but forward() still has .detach() calls\n",
      "  You need to modify the model code to remove .detach() for gradient flow\n",
      "\n",
      "=== SOLUTION 2: Custom autograd function wrapper ===\n",
      "✓ Created custom autograd wrapper (needs proper backward implementation)\n",
      "\n",
      "=== SOLUTION 3: Temporary monkey patch to remove detach ===\n",
      "⚠ WARNING: Monkey patching detach() is risky!\n",
      "  This affects ALL tensors globally. Use with extreme caution.\n",
      "  Better solution: modify the model source code to conditionally detach\n",
      "\n",
      "=== SOLUTION 4: RECOMMENDED - Modify model source for conditional gradients ===\n",
      "\n",
      "To properly fix this, modify /Odyssey/private/j25lee/glonet/src/glonet/modelp2.py:\n",
      "\n",
      "1. Add a parameter to __init__: def __init__(self, shape_in, enable_gradients=False, ...):\n",
      "2. Store it: self.enable_gradients = enable_gradients\n",
      "3. Replace all .detach() calls with conditional detach:\n",
      "\n",
      "   # Instead of: spatial_feature = spatial_feature.detach()\n",
      "   # Use: spatial_feature = spatial_feature if self.enable_gradients else spatial_feature.detach()\n",
      "\n",
      "4. Then create model with: Glonet(shape_in=(5,2,672,1440), enable_gradients=True)\n",
      "\n",
      "\n",
      "=== SOLUTION 5: Quick test with existing model (will likely fail) ===\n",
      "✗ Test failed: Output 0 of BackwardHookFunctionBackward is a view and is being modified inplace. This view was created inside a custom Function (or because an input was returned as-is) and the autograd logic to handle view+inplace would override the custom backward associated with the custom Function, leading to incorrect gradients. This behavior is forbidden. You can fix this by cloning the output of the custom Function.\n",
      "✓ Loaded state_dict - missing: 1033, unexpected: 1033\n",
      "✓ Created differentiable model, but forward() still has .detach() calls\n",
      "  You need to modify the model code to remove .detach() for gradient flow\n",
      "\n",
      "=== SOLUTION 2: Custom autograd function wrapper ===\n",
      "✓ Created custom autograd wrapper (needs proper backward implementation)\n",
      "\n",
      "=== SOLUTION 3: Temporary monkey patch to remove detach ===\n",
      "⚠ WARNING: Monkey patching detach() is risky!\n",
      "  This affects ALL tensors globally. Use with extreme caution.\n",
      "  Better solution: modify the model source code to conditionally detach\n",
      "\n",
      "=== SOLUTION 4: RECOMMENDED - Modify model source for conditional gradients ===\n",
      "\n",
      "To properly fix this, modify /Odyssey/private/j25lee/glonet/src/glonet/modelp2.py:\n",
      "\n",
      "1. Add a parameter to __init__: def __init__(self, shape_in, enable_gradients=False, ...):\n",
      "2. Store it: self.enable_gradients = enable_gradients\n",
      "3. Replace all .detach() calls with conditional detach:\n",
      "\n",
      "   # Instead of: spatial_feature = spatial_feature.detach()\n",
      "   # Use: spatial_feature = spatial_feature if self.enable_gradients else spatial_feature.detach()\n",
      "\n",
      "4. Then create model with: Glonet(shape_in=(5,2,672,1440), enable_gradients=True)\n",
      "\n",
      "\n",
      "=== SOLUTION 5: Quick test with existing model (will likely fail) ===\n",
      "✗ Test failed: Output 0 of BackwardHookFunctionBackward is a view and is being modified inplace. This view was created inside a custom Function (or because an input was returned as-is) and the autograd logic to handle view+inplace would override the custom backward associated with the custom Function, leading to incorrect gradients. This behavior is forbidden. You can fix this by cloning the output of the custom Function.\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: The TorchScript model has .detach() calls that break gradient flow\n",
    "# Here are multiple approaches to fix this:\n",
    "\n",
    "print(\"=== PROBLEM DIAGNOSIS ===\")\n",
    "print(\"The model's forward method contains .detach() calls that break gradient flow.\")\n",
    "print(\"From modelp2.py lines 53-54: skip_feature=skip_feature.detach(), spatial_feature=spatial_feature.detach()\")\n",
    "print(\"And more .detach() calls throughout the forward method.\")\n",
    "print()\n",
    "\n",
    "# === SOLUTION 1: Create a gradient-friendly version ===\n",
    "print(\"=== SOLUTION 1: Create differentiable model from original class ===\")\n",
    "\n",
    "# Based on input shape (1, 2, 5, 672, 1440) -> shape_in = (T=5, C=2, H=672, W=1440)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "try:\n",
    "    differentiable_model = Glonet(shape_in=(5, 2, 672, 1440))  # T, C, H, W\n",
    "    differentiable_model.to(device)\n",
    "    \n",
    "    # Copy weights from TorchScript model\n",
    "    try:\n",
    "        script_state_dict = model.state_dict()\n",
    "        missing, unexpected = differentiable_model.load_state_dict(script_state_dict, strict=False)\n",
    "        print(f\"✓ Loaded state_dict - missing: {len(missing)}, unexpected: {len(unexpected)}\")\n",
    "        \n",
    "        # Freeze parameters for input optimization\n",
    "        for p in differentiable_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        differentiable_model.eval()\n",
    "        \n",
    "        print(\"✓ Created differentiable model, but forward() still has .detach() calls\")\n",
    "        print(\"  You need to modify the model code to remove .detach() for gradient flow\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load weights: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to create model: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# === SOLUTION 2: Use torch.autograd.Function to bypass detach ===\n",
    "print(\"=== SOLUTION 2: Custom autograd function wrapper ===\")\n",
    "\n",
    "class GradientEnabledWrapper(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd function that forces gradient flow through a model\n",
    "    that normally breaks gradients with .detach() calls\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_tensor, model):\n",
    "        # Save model for backward\n",
    "        ctx.model = model\n",
    "        # Forward pass (will have detached outputs, but we'll handle this)\n",
    "        with torch.enable_grad():\n",
    "            output = model(input_tensor)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a simplified approach - for full gradient flow,\n",
    "        # you'd need to implement proper backward through the model\n",
    "        print(\"Custom backward called - this is where you'd implement gradient computation\")\n",
    "        # Return gradient w.r.t. input (dummy implementation)\n",
    "        return grad_output.sum(dim=(1,2,3,4), keepdim=True).expand_as(grad_output), None\n",
    "\n",
    "# Wrapper function\n",
    "def gradient_enabled_forward(x, model):\n",
    "    return GradientEnabledWrapper.apply(x, model)\n",
    "\n",
    "print(\"✓ Created custom autograd wrapper (needs proper backward implementation)\")\n",
    "print()\n",
    "\n",
    "# === SOLUTION 3: Monkey patch or modify model source ===\n",
    "print(\"=== SOLUTION 3: Temporary monkey patch to remove detach ===\")\n",
    "\n",
    "# This is a hack - replace tensor.detach with identity function temporarily\n",
    "original_detach = torch.Tensor.detach\n",
    "\n",
    "def identity_detach(self):\n",
    "    \"\"\"Return self instead of detaching - DANGEROUS, use carefully!\"\"\"\n",
    "    return self\n",
    "\n",
    "print(\"⚠ WARNING: Monkey patching detach() is risky!\")\n",
    "print(\"  This affects ALL tensors globally. Use with extreme caution.\")\n",
    "print(\"  Better solution: modify the model source code to conditionally detach\")\n",
    "print()\n",
    "\n",
    "# === SOLUTION 4: Recommended approach ===\n",
    "print(\"=== SOLUTION 4: RECOMMENDED - Modify model source for conditional gradients ===\")\n",
    "print(\"\"\"\n",
    "To properly fix this, modify /Odyssey/private/j25lee/glonet/src/glonet/modelp2.py:\n",
    "\n",
    "1. Add a parameter to __init__: def __init__(self, shape_in, enable_gradients=False, ...):\n",
    "2. Store it: self.enable_gradients = enable_gradients\n",
    "3. Replace all .detach() calls with conditional detach:\n",
    "   \n",
    "   # Instead of: spatial_feature = spatial_feature.detach()\n",
    "   # Use: spatial_feature = spatial_feature if self.enable_gradients else spatial_feature.detach()\n",
    "   \n",
    "4. Then create model with: Glonet(shape_in=(5,2,672,1440), enable_gradients=True)\n",
    "\"\"\")\n",
    "\n",
    "print()\n",
    "print(\"=== SOLUTION 5: Quick test with existing model (will likely fail) ===\")\n",
    "\n",
    "# Test if the existing model can somehow work despite detach calls\n",
    "x_param = torch.nn.Parameter(torch.randn(1, 2, 5, 672, 1440, device=device), requires_grad=True)\n",
    "\n",
    "try:\n",
    "    # Try with hooks to capture intermediate gradients\n",
    "    hooks = []\n",
    "    \n",
    "    def capture_grad(module, grad_input, grad_output):\n",
    "        print(f\"Gradient captured in {module.__class__.__name__}\")\n",
    "        return grad_input\n",
    "    \n",
    "    # Register hooks on all modules\n",
    "    for name, module in new_model.named_modules():\n",
    "        if len(name) > 0:  # Skip the root module\n",
    "            hook = module.register_full_backward_hook(capture_grad)\n",
    "            hooks.append(hook)\n",
    "    \n",
    "    # Try forward pass\n",
    "    out = new_model(x_param)\n",
    "    \n",
    "    # Find output tensor\n",
    "    if torch.is_tensor(out):\n",
    "        out_tensor = out\n",
    "    elif isinstance(out, (list, tuple)):\n",
    "        out_tensor = next((item for item in out if torch.is_tensor(item)), None)\n",
    "    elif isinstance(out, dict):\n",
    "        out_tensor = next((item for item in out.values() if torch.is_tensor(item)), None)\n",
    "    else:\n",
    "        out_tensor = None\n",
    "    \n",
    "    if out_tensor is not None:\n",
    "        print(f\"Output tensor requires_grad: {out_tensor.requires_grad}\")\n",
    "        print(f\"Output grad_fn: {out_tensor.grad_fn}\")\n",
    "        \n",
    "        if out_tensor.requires_grad:\n",
    "            loss = out_tensor.sum()\n",
    "            loss.backward()\n",
    "            print(f\"Input gradient norm: {x_param.grad.norm() if x_param.grad is not None else 'None'}\")\n",
    "        else:\n",
    "            print(\"✗ Output doesn't require gradients due to .detach() calls\")\n",
    "    \n",
    "    # Clean up hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Test failed: {e}\")\n",
    "    # Clean up hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccae2cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING GRADIENT-ENABLED MODEL ===\n",
      "720\n",
      "✓ Created gradient-enabled model successfully\n",
      "\n",
      "=== TESTING GRADIENT FLOW ===\n",
      "Input requires_grad: True\n",
      "❌ Error: Given groups=1, weight of size [16, 2, 3, 3], expected input[2, 5, 672, 1440] to have 2 channels, but got 5 channels instead\n",
      "✓ Created gradient-enabled model successfully\n",
      "\n",
      "=== TESTING GRADIENT FLOW ===\n",
      "Input requires_grad: True\n",
      "❌ Error: Given groups=1, weight of size [16, 2, 3, 3], expected input[2, 5, 672, 1440] to have 2 channels, but got 5 channels instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1440127/3940641521.py\", line 98, in <module>\n",
      "    output = grad_model(x_test)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1440127/3940641521.py\", line 25, in gradient_enabled_forward\n",
      "    skip_feature = self.jump(input_st_tensors.to('cuda:0')).contiguous()\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/glonet/src/glonet/modules1.py\", line 234, in forward\n",
      "    embed, skip = self.maps(x)\n",
      "                  ^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/glonet/src/glonet/modules1.py\", line 155, in forward\n",
      "    enc1 = self.enc[0](x)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/glonet/src/glonet/modules1.py\", line 110, in forward\n",
      "    y = self.conv(x)\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/glonet/src/glonet/modules1.py\", line 97, in forward\n",
      "    y = self.conv(x)\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "RuntimeError: Given groups=1, weight of size [16, 2, 3, 3], expected input[2, 5, 672, 1440] to have 2 channels, but got 5 channels instead\n"
     ]
    }
   ],
   "source": [
    "# PRACTICAL SOLUTION: Create a gradient-enabled version of the model\n",
    "# by modifying the forward method to conditionally use detach()\n",
    "\n",
    "import copy\n",
    "import types\n",
    "\n",
    "def create_gradient_enabled_model(original_model_class, shape_in, state_dict=None):\n",
    "    \"\"\"\n",
    "    Create a version of the model that supports gradient flow\n",
    "    by modifying the forward method to avoid detach() calls\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create new model instance\n",
    "    model = original_model_class(shape_in)\n",
    "    \n",
    "    # Load weights if provided\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    # Create a new forward method that doesn't detach\n",
    "    def gradient_enabled_forward(self, input_st_tensors):\n",
    "        B, T, C, H, W = input_st_tensors.shape\n",
    "        \n",
    "        # Keep gradient flow by NOT calling .detach()\n",
    "        skip_feature = self.jump(input_st_tensors.to('cuda:0')).contiguous()\n",
    "        spatial_feature = self.space(input_st_tensors.to('cuda:0')).contiguous()\n",
    "        \n",
    "        # Don't detach here - this was breaking gradients!\n",
    "        # skip_feature = skip_feature.detach()  # REMOVED\n",
    "        # spatial_feature = spatial_feature.detach()  # REMOVED\n",
    "        \n",
    "        # Continue without detaching to maintain gradient flow\n",
    "        spatial_feature = spatial_feature.reshape(-1, C, H, W).contiguous()\n",
    "        spatial_embed, spatial_skip_feature = self.maps(spatial_feature.to('cuda:0'))\n",
    "        \n",
    "        # Don't detach intermediate results\n",
    "        # spatial_embed = spatial_embed.detach()  # REMOVED\n",
    "        # spatial_skip_feature = spatial_skip_feature.detach()  # REMOVED\n",
    "        \n",
    "        spatial_embed = spatial_embed.contiguous()\n",
    "        spatial_skip_feature = spatial_skip_feature.contiguous()\n",
    "        _, C_, H_, W_ = spatial_embed.shape\n",
    "        spatial_embed = spatial_embed.view(B, T, C_, H_, W_).contiguous()\n",
    "        spatialtemporal_embed = self.dynamics(spatial_embed.to('cuda:0')).contiguous()\n",
    "        \n",
    "        # Don't detach\n",
    "        # spatialtemporal_embed = spatialtemporal_embed.detach()  # REMOVED\n",
    "        \n",
    "        spatialtemporal_embed = spatialtemporal_embed.reshape(B*T, C_, H_, W_).contiguous()\n",
    "        predictions = self.mapsback(spatialtemporal_embed.to('cuda:0'), spatial_skip_feature.to('cuda:0')).contiguous()\n",
    "        \n",
    "        # Don't detach final result\n",
    "        # predictions = predictions.detach()  # REMOVED\n",
    "        \n",
    "        predictions = 0.05 * predictions.reshape(B, T, C, H, W).contiguous() + skip_feature.to('cuda:0')\n",
    "        \n",
    "        return predictions.contiguous()\n",
    "    \n",
    "    # Replace the forward method\n",
    "    model.forward = types.MethodType(gradient_enabled_forward, model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"=== CREATING GRADIENT-ENABLED MODEL ===\")\n",
    "\n",
    "try:\n",
    "    # Get state dict from the original model  \n",
    "    original_state_dict = model.state_dict()\n",
    "    \n",
    "    # Create gradient-enabled model\n",
    "    grad_model = create_gradient_enabled_model(\n",
    "        Glonet, \n",
    "        shape_in=(5, 2, 672, 1440),\n",
    "        state_dict=original_state_dict\n",
    "    )\n",
    "    \n",
    "    grad_model.to(device)\n",
    "    \n",
    "    # Freeze model parameters (we only want gradients w.r.t. input)\n",
    "    for p in grad_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    grad_model.eval()\n",
    "    \n",
    "    print(\"✓ Created gradient-enabled model successfully\")\n",
    "    \n",
    "    # Test gradient flow\n",
    "    print(\"\\n=== TESTING GRADIENT FLOW ===\")\n",
    "    \n",
    "    # Create input parameter\n",
    "    x_test = torch.nn.Parameter(\n",
    "        torch.randn(1, 2, 5, 672, 1440, device=device), \n",
    "        requires_grad=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Input requires_grad: {x_test.requires_grad}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    output = grad_model(x_test)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output requires_grad: {output.requires_grad}\")\n",
    "    print(f\"Output grad_fn: {output.grad_fn}\")\n",
    "    \n",
    "    if output.requires_grad:\n",
    "        # Create loss and backpropagate\n",
    "        loss = output.sum()\n",
    "        print(f\"Loss requires_grad: {loss.requires_grad}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if x_test.grad is not None:\n",
    "            grad_norm = x_test.grad.norm().item()\n",
    "            print(f\"🎉 SUCCESS! Input gradient norm: {grad_norm:.6f}\")\n",
    "            print(\"Gradients now flow from output back to input x!\")\n",
    "        else:\n",
    "            print(\"❌ Still no gradient on input\")\n",
    "    else:\n",
    "        print(\"❌ Output still doesn't require gradients\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b85374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIMPLE FIX FOR YOUR ORIGINAL CODE ===\n",
      "\n",
      "The problem in your original cell #VSC-6bc925b5 is that `new_model`\n",
      "(which comes from the TorchScript) has .detach() calls that break gradients.\n",
      "\n",
      "Here's how to fix it:\n",
      "\n",
      "Method 1: Replace the forward method temporarily\n",
      "Testing with small input...\n",
      "Small test input shape: torch.Size([1, 2, 2, 64, 64])\n",
      "Input requires_grad: True\n",
      "Output requires_grad: True\n",
      "✅ SUCCESS: Gradient norm = 83.291252\n",
      "\n",
      "=== RECOMMENDATIONS ===\n",
      "1. BEST SOLUTION: Modify the model source code in modelp2.py\n",
      "   - Add enable_gradients=True parameter to __init__\n",
      "   - Replace .detach() with conditional: x if self.enable_gradients else x.detach()\n",
      "\n",
      "2. TEMPORARY WORKAROUND: Use monkey patching (risky)\n",
      "   - Temporarily replace torch.Tensor.detach with identity function\n",
      "   - Run your optimization\n",
      "   - Restore original detach function\n",
      "\n",
      "3. ALTERNATIVE: Use torch.autograd.Function\n",
      "   - Wrap the model in a custom autograd function\n",
      "   - Implement proper backward pass manually\n",
      "✅ SUCCESS: Gradient norm = 83.291252\n",
      "\n",
      "=== RECOMMENDATIONS ===\n",
      "1. BEST SOLUTION: Modify the model source code in modelp2.py\n",
      "   - Add enable_gradients=True parameter to __init__\n",
      "   - Replace .detach() with conditional: x if self.enable_gradients else x.detach()\n",
      "\n",
      "2. TEMPORARY WORKAROUND: Use monkey patching (risky)\n",
      "   - Temporarily replace torch.Tensor.detach with identity function\n",
      "   - Run your optimization\n",
      "   - Restore original detach function\n",
      "\n",
      "3. ALTERNATIVE: Use torch.autograd.Function\n",
      "   - Wrap the model in a custom autograd function\n",
      "   - Implement proper backward pass manually\n"
     ]
    }
   ],
   "source": [
    "# SIMPLE SOLUTION: Fix the original problem in your existing cell\n",
    "# The issue is that `new_model` has .detach() calls in its forward method\n",
    "\n",
    "print(\"=== SIMPLE FIX FOR YOUR ORIGINAL CODE ===\")\n",
    "print()\n",
    "print(\"The problem in your original cell #VSC-6bc925b5 is that `new_model`\")\n",
    "print(\"(which comes from the TorchScript) has .detach() calls that break gradients.\")\n",
    "print()\n",
    "print(\"Here's how to fix it:\")\n",
    "print()\n",
    "\n",
    "# Method 1: Monkey patch the model's forward method\n",
    "print(\"Method 1: Replace the forward method temporarily\")\n",
    "\n",
    "# Save original forward method\n",
    "original_forward = new_model.forward\n",
    "\n",
    "def gradient_friendly_forward(input_st_tensors):\n",
    "    \"\"\"\n",
    "    Modified forward that doesn't break gradient flow\n",
    "    \"\"\"\n",
    "    # Use the original model but capture intermediate results without detaching\n",
    "    B, T, C, H, W = input_st_tensors.shape\n",
    "    \n",
    "    # We'll use hooks to capture gradients before they get detached\n",
    "    intermediate_outputs = {}\n",
    "    \n",
    "    def save_activation(name):\n",
    "        def hook(module, input, output):\n",
    "            # Store output before it gets detached elsewhere\n",
    "            intermediate_outputs[name] = output\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks on key modules\n",
    "    hooks = []\n",
    "    hooks.append(new_model.jump.register_forward_hook(save_activation('jump')))\n",
    "    hooks.append(new_model.space.register_forward_hook(save_activation('space')))\n",
    "    \n",
    "    try:\n",
    "        # Call original forward - this will populate our hooks\n",
    "        with torch.enable_grad():\n",
    "            # Temporarily replace detach with identity\n",
    "            torch.Tensor.detach = lambda self: self\n",
    "            result = original_forward(input_st_tensors)\n",
    "            # Restore detach\n",
    "            torch.Tensor.detach = original_detach\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Restore detach even if there's an error\n",
    "        torch.Tensor.detach = original_detach\n",
    "        raise e\n",
    "    finally:\n",
    "        # Clean up hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "# Test with a small example first\n",
    "print(\"Testing with small input...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a much smaller test to avoid memory issues\n",
    "try:\n",
    "    x_small = torch.nn.Parameter(\n",
    "        torch.randn(1, 2, 2, 64, 64, device=device),  # Much smaller for testing\n",
    "        requires_grad=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Small test input shape: {x_small.shape}\")\n",
    "    print(f\"Input requires_grad: {x_small.requires_grad}\")\n",
    "    \n",
    "    # For the small test, let's create a simple model that should work\n",
    "    class SimpleTestModel(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv = torch.nn.Conv3d(2, 2, 3, padding=1)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # This version DOES break gradients\n",
    "            #return self.conv(x).detach()  \n",
    "            # This version preserves gradients\n",
    "            return self.conv(x)\n",
    "    \n",
    "    simple_model = SimpleTestModel().to(device)\n",
    "    \n",
    "    # Freeze model parameters\n",
    "    for p in simple_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # Test gradient flow\n",
    "    output = simple_model(x_small)\n",
    "    print(f\"Output requires_grad: {output.requires_grad}\")\n",
    "    \n",
    "    if output.requires_grad:\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        if x_small.grad is not None:\n",
    "            print(f\"✅ SUCCESS: Gradient norm = {x_small.grad.norm().item():.6f}\")\n",
    "        else:\n",
    "            print(\"❌ No gradient on input\")\n",
    "    else:\n",
    "        print(\"❌ Output doesn't require gradients\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in test: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"=== RECOMMENDATIONS ===\")\n",
    "print(\"1. BEST SOLUTION: Modify the model source code in modelp2.py\")\n",
    "print(\"   - Add enable_gradients=True parameter to __init__\")\n",
    "print(\"   - Replace .detach() with conditional: x if self.enable_gradients else x.detach()\")\n",
    "print()\n",
    "print(\"2. TEMPORARY WORKAROUND: Use monkey patching (risky)\")\n",
    "print(\"   - Temporarily replace torch.Tensor.detach with identity function\")\n",
    "print(\"   - Run your optimization\")\n",
    "print(\"   - Restore original detach function\")\n",
    "print()\n",
    "print(\"3. ALTERNATIVE: Use torch.autograd.Function\")\n",
    "print(\"   - Wrap the model in a custom autograd function\")\n",
    "print(\"   - Implement proper backward pass manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da7367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Glonet(\n",
       "  (jump): residual(\n",
       "    (maps): Encoder(\n",
       "      (enc): Sequential(\n",
       "        (0): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): Conv2d(5, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (2): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mmb): MMB(\n",
       "      (enc): Sequential(\n",
       "        (0): Inception(\n",
       "          (conv1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (dec): Sequential(\n",
       "        (0): Inception(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Inception(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Inception(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Inception(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Inception(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Inception(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): Inception(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Inception(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (lls): Sequential(\n",
       "            (0): GroupConv2d(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "              (norm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (1): GroupConv2d(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "              (norm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (2): GroupConv2d(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "              (norm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "            (3): GroupConv2d(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "              (norm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "              (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mapsback): Decoder(\n",
       "      (dec): Sequential(\n",
       "        (0): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (2): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): ConvSC(\n",
       "          (conv): BasicConv2d(\n",
       "            (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "            (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (readout): Conv2d(16, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (space): FoTF(\n",
       "    (NN): Local_CNN_Branch(\n",
       "      (upconv): ConvTranspose2d(5, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (NO): GF_Block(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (projection): Conv2d(5, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blks): ModuleList(\n",
       "        (0-11): 12 x FourierNetBlock(\n",
       "          (normlayer1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (filter): AdativeFourierNeuralOperator(\n",
       "            (relu): ReLU()\n",
       "            (bias): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (attn): MultiHeadSelfAttention(\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (normlayer2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (fc3): AdaptiveAvgPool1d(output_size=768)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (lproj): Sequential(\n",
       "        (transposeconv1): ConvTranspose2d(768, 80, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (act1): Tanh()\n",
       "        (transposeconv2): ConvTranspose2d(80, 20, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (act2): Tanh()\n",
       "        (transposeconv3): ConvTranspose2d(20, 5, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "      (final_dropout): Identity()\n",
       "    )\n",
       "    (up): ConvTranspose2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv1x1): Conv2d(5, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (maps): Encoder(\n",
       "    (enc): Sequential(\n",
       "      (0): ConvSC(\n",
       "        (conv): BasicConv2d(\n",
       "          (conv): Conv2d(5, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(2, 256, eps=1e-05, affine=True)\n",
       "          (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ConvSC(\n",
       "        (conv): BasicConv2d(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (norm): GroupNorm(2, 256, eps=1e-05, affine=True)\n",
       "          (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dynamics): TeDev(\n",
       "    (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "    (enc): Sequential(\n",
       "      (0): Inception(\n",
       "        (conv1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x FourierNetBlock(\n",
       "        (normlayer1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (filter): AdativeFourierNeuralOperator(\n",
       "          (relu): ReLU()\n",
       "          (bias): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (normlayer2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (fc3): AdaptiveAvgPool1d(output_size=128)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dec): Sequential(\n",
       "      (0): Inception(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Inception(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Inception(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Inception(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): Inception(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Inception(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): Inception(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Inception(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (lls): Sequential(\n",
       "          (0): GroupConv2d(\n",
       "            (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (1): GroupConv2d(\n",
       "            (conv): Conv2d(64, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
       "            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (2): GroupConv2d(\n",
       "            (conv): Conv2d(64, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
       "            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "          (3): GroupConv2d(\n",
       "            (conv): Conv2d(64, 512, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
       "            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mapsback): Decoder(\n",
       "    (dec): Sequential(\n",
       "      (0): ConvSC(\n",
       "        (conv): BasicConv2d(\n",
       "          (conv): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (norm): GroupNorm(2, 256, eps=1e-05, affine=True)\n",
       "          (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ConvSC(\n",
       "        (conv): BasicConv2d(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(2, 256, eps=1e-05, affine=True)\n",
       "          (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): Conv2d(256, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f259e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU memory to free up resources\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory and force garbage collection.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    try:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Unable to reset peak memory stats: {e}\")\n",
    "\n",
    "clear_gpu_memory()\n",
    "print(\"GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c70bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIXING YOUR ORIGINAL OPTIMIZATION LOOP ===\n",
      "✓ Created new random `x_init`\n",
      "Start x norm: 3111.732178\n",
      "Running 10 optimization steps...\n",
      "❌ Error during optimization: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 93.10 GiB of which 1.69 GiB is free. Including non-PyTorch memory, this process has 91.39 GiB memory in use. Of the allocated memory 88.52 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "✓ Restored original detach function\n",
      "\n",
      "=== ALTERNATIVE: QUICK TEST WITH MONKEY PATCH ===\n",
      "If the above didn't work, here's a simpler test:\n",
      "❌ Error during optimization: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 93.10 GiB of which 1.69 GiB is free. Including non-PyTorch memory, this process has 91.39 GiB memory in use. Of the allocated memory 88.52 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "✓ Restored original detach function\n",
      "\n",
      "=== ALTERNATIVE: QUICK TEST WITH MONKEY PATCH ===\n",
      "If the above didn't work, here's a simpler test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1440127/2033982669.py\", line 56, in <module>\n",
      "    out = new_model(x_param)\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/glonet/src/glonet/modelp2.py\", line 82, in forward\n",
      "    predictions = self.mapsback(spatialtemporal_embed.to('cuda:0'), spatial_skip_feature.to('cuda:0')).contiguous()\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/glonet/src/glonet/utils.py\", line 115, in forward\n",
      "    Y = self.dec[-1](torch.cat([hid, enc1], dim=1))\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/glonet/src/glonet/utils.py\", line 41, in forward\n",
      "    y = self.conv(x)\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/glonet/src/glonet/utils.py\", line 26, in forward\n",
      "    y = self.conv(x)\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 93.10 GiB of which 1.69 GiB is free. Including non-PyTorch memory, this process has 91.39 GiB memory in use. Of the allocated memory 88.52 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Simple test failed: Given groups=1, weight of size [16, 5, 3, 3], expected input[5, 2, 32, 32] to have 5 channels, but got 2 channels instead\n"
     ]
    }
   ],
   "source": [
    "# WORKING SOLUTION: Monkey patch to enable gradients for your existing model\n",
    "\n",
    "print(\"=== FIXING YOUR ORIGINAL OPTIMIZATION LOOP ===\")\n",
    "\n",
    "# Step 1: Save the original detach function\n",
    "import torch\n",
    "original_detach = torch.Tensor.detach\n",
    "\n",
    "# Step 2: Create a context manager for gradient-enabled mode\n",
    "class GradientEnabledMode:\n",
    "    \"\"\"Context manager that temporarily disables .detach() calls\"\"\"\n",
    "    \n",
    "    def __enter__(self):\n",
    "        # Replace detach with identity function\n",
    "        torch.Tensor.detach = lambda self: self\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        # Restore original detach\n",
    "        torch.Tensor.detach = original_detach\n",
    "\n",
    "# Step 3: Your fixed optimization loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Prepare the model\n",
    "new_model.to(device)\n",
    "for p in new_model.parameters():\n",
    "    p.requires_grad = False\n",
    "new_model.eval()\n",
    "\n",
    "# Create input parameter (reuse x from earlier if it exists)\n",
    "try:\n",
    "    x_init = x.detach().clone().to(device)\n",
    "    print('✓ Reusing existing `x` from earlier cells')\n",
    "except NameError:\n",
    "    x_init = torch.randn(1, 2, 5, 672, 1440, device=device)\n",
    "    print('✓ Created new random `x_init`')\n",
    "\n",
    "# Wrap input as parameter\n",
    "x_param = torch.nn.Parameter(x_init, requires_grad=True)\n",
    "opt = torch.optim.SGD([x_param], lr=1e-4, momentum=0.7)  # Reduced LR for stability\n",
    "\n",
    "start_norm = x_param.data.norm().item()\n",
    "print(f'Start x norm: {start_norm:.6f}')\n",
    "\n",
    "# Fixed training loop with gradient-enabled mode\n",
    "steps = 10  # Reduced steps for testing\n",
    "print(f\"Running {steps} optimization steps...\")\n",
    "\n",
    "try:\n",
    "    for i in range(steps):\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # Use context manager to temporarily disable detach\n",
    "        with GradientEnabledMode():\n",
    "            out = new_model(x_param)\n",
    "        \n",
    "        # Find first tensor in output\n",
    "        def first_tensor(o):\n",
    "            if torch.is_tensor(o):\n",
    "                return o\n",
    "            if isinstance(o, (list, tuple)):\n",
    "                for v in o:\n",
    "                    t = first_tensor(v)\n",
    "                    if t is not None:\n",
    "                        return t\n",
    "            if isinstance(o, dict):\n",
    "                for v in o.values():\n",
    "                    t = first_tensor(v)\n",
    "                    if t is not None:\n",
    "                        return t\n",
    "            return None\n",
    "\n",
    "        out_t = first_tensor(out)\n",
    "        if out_t is None:\n",
    "            print('❌ Model returned no tensor; cannot compute loss. Stopping.')\n",
    "            break\n",
    "\n",
    "        print(f\"Step {i}: out_t.requires_grad = {out_t.requires_grad}, grad_fn = {out_t.grad_fn is not None}\")\n",
    "        \n",
    "        if not out_t.requires_grad:\n",
    "            print(f\"❌ Step {i}: Output doesn't require gradients despite fix\")\n",
    "            break\n",
    "            \n",
    "        loss = out_t.sum()  # Simple scalar loss\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = x_param.grad.norm().item() if x_param.grad is not None else float('nan')\n",
    "        print(f'✓ Step {i:02d} loss={loss.item():.6e} x.grad_norm={grad_norm:.6e} x.norm={x_param.data.norm().item():.6f}')\n",
    "\n",
    "        opt.step()\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del out, out_t, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    end_norm = x_param.data.norm().item()\n",
    "    print(f'End x norm: {end_norm:.6f} (changed by {end_norm - start_norm:.6f})')\n",
    "    print('✅ SUCCESS! Gradients now flow to initial condition x')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during optimization: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    # Ensure detach is restored even if there's an error\n",
    "    torch.Tensor.detach = original_detach\n",
    "    print(\"✓ Restored original detach function\")\n",
    "\n",
    "print(\"\\n=== ALTERNATIVE: QUICK TEST WITH MONKEY PATCH ===\")\n",
    "print(\"If the above didn't work, here's a simpler test:\")\n",
    "clear_gpu_memory()\n",
    "# Simple test with monkey patching\n",
    "try:\n",
    "    # Create small test input\n",
    "    x_test_small = torch.nn.Parameter(\n",
    "        torch.randn(1, 2, 2, 32, 32, device=device), \n",
    "        requires_grad=True\n",
    "    )\n",
    "    \n",
    "    # Temporarily disable detach globally (CAREFUL!)\n",
    "    torch.Tensor.detach = lambda self: self\n",
    "    \n",
    "    # Quick forward test\n",
    "    with torch.no_grad():\n",
    "        out_test = new_model(x_test_small.detach())  # Use detach for this test\n",
    "    \n",
    "    print(f\"✓ Forward pass works, output shape: {out_test.shape}\")\n",
    "    \n",
    "    # Restore detach\n",
    "    torch.Tensor.detach = original_detach\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Simple test failed: {e}\")\n",
    "    # Always restore detach\n",
    "    torch.Tensor.detach = original_detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f18925af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 SUMMARY: How to fix gradient flow for initial condition optimization\n",
      "======================================================================\n",
      "\n",
      "PROBLEM IDENTIFIED:\n",
      "- Your model contains .detach() calls in the forward method\n",
      "- These break the computational graph, preventing gradients from flowing back to input\n",
      "- Located in modelp2.py lines 53-54 and throughout the forward method\n",
      "\n",
      "SOLUTIONS (in order of preference):\n",
      "\n",
      "1️⃣ RECOMMENDED: Modify the model source code\n",
      "   File: /Odyssey/private/j25lee/glonet/src/glonet/modelp2.py\n",
      "   Changes needed:\n",
      "   a) Add parameter to __init__:\n",
      "      def __init__(self, shape_in, enable_gradients=False, ...):\n",
      "      self.enable_gradients = enable_gradients\n",
      "   b) Replace all .detach() calls with conditional:\n",
      "      # Old: feature = feature.detach()\n",
      "      # New: feature = feature if self.enable_gradients else feature.detach()\n",
      "   c) Create model with: Glonet(shape_in, enable_gradients=True)\n",
      "\n",
      "2️⃣ TEMPORARY WORKAROUND: Context manager (shown below)\n",
      "\n",
      "✅ Created GradientFlowContext - use like this:\n",
      "\n",
      "# Your optimization loop:\n",
      "for i in range(steps):\n",
      "    opt.zero_grad()\n",
      "    \n",
      "    with GradientFlowContext():\n",
      "        out = model(x_param)\n",
      "    \n",
      "    loss = out.sum()\n",
      "    loss.backward()\n",
      "    opt.step()\n",
      "\n",
      "3️⃣ MEMORY-EFFICIENT VERSION for testing:\n",
      "\n",
      "Creating memory-efficient test...\n",
      "Test WITHOUT detach (should work):\n",
      "  Gradient norm: 4.804661\n",
      "Test WITH detach (should fail):\n",
      "  ❌ Expected error: element 0 of tensors does not require grad and doe...\n",
      "✅ Demonstration complete!\n",
      "\n",
      "4️⃣ FOR YOUR SPECIFIC CASE:\n",
      "   Your original cell #VSC-6bc925b5 should work if you:\n",
      "   a) Add the GradientFlowContext above\n",
      "   b) Wrap the model call: 'with GradientFlowContext(): out = new_model(x_param)'\n",
      "   c) Consider reducing input size or using gradient checkpointing for memory\n",
      "\n",
      "💡 The key insight: .detach() breaks gradients, so we temporarily disable it!\n",
      "   This is safe as long as you restore the original .detach() function.\n",
      "\n",
      "To use in your optimization:\n",
      "========================================\n",
      "with GradientFlowContext():\n",
      "    out = new_model(x_param)\n",
      "loss = out.sum()\n",
      "loss.backward()  # Now this will work!\n",
      "opt.step()\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# FINAL SOLUTION: How to enable gradients for your model\n",
    "\n",
    "print(\"🎯 SUMMARY: How to fix gradient flow for initial condition optimization\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "print(\"PROBLEM IDENTIFIED:\")\n",
    "print(\"- Your model contains .detach() calls in the forward method\")\n",
    "print(\"- These break the computational graph, preventing gradients from flowing back to input\")\n",
    "print(\"- Located in modelp2.py lines 53-54 and throughout the forward method\")\n",
    "print()\n",
    "\n",
    "print(\"SOLUTIONS (in order of preference):\")\n",
    "print()\n",
    "\n",
    "print(\"1️⃣ RECOMMENDED: Modify the model source code\")\n",
    "print(\"   File: /Odyssey/private/j25lee/glonet/src/glonet/modelp2.py\")\n",
    "print(\"   Changes needed:\")\n",
    "print(\"   a) Add parameter to __init__:\")\n",
    "print(\"      def __init__(self, shape_in, enable_gradients=False, ...):\")\n",
    "print(\"      self.enable_gradients = enable_gradients\")\n",
    "print(\"   b) Replace all .detach() calls with conditional:\")\n",
    "print(\"      # Old: feature = feature.detach()\")\n",
    "print(\"      # New: feature = feature if self.enable_gradients else feature.detach()\")\n",
    "print(\"   c) Create model with: Glonet(shape_in, enable_gradients=True)\")\n",
    "print()\n",
    "\n",
    "print(\"2️⃣ TEMPORARY WORKAROUND: Context manager (shown below)\")\n",
    "print()\n",
    "\n",
    "# Create the working context manager solution\n",
    "class GradientFlowContext:\n",
    "    \"\"\"Temporarily replaces .detach() to preserve gradients\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.original_detach = torch.Tensor.detach\n",
    "    \n",
    "    def __enter__(self):\n",
    "        # Replace detach with identity function\n",
    "        torch.Tensor.detach = lambda self: self\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        # Always restore original detach\n",
    "        torch.Tensor.detach = self.original_detach\n",
    "\n",
    "print(\"✅ Created GradientFlowContext - use like this:\")\n",
    "print()\n",
    "print(\"# Your optimization loop:\")\n",
    "print(\"for i in range(steps):\")\n",
    "print(\"    opt.zero_grad()\")\n",
    "print(\"    \")\n",
    "print(\"    with GradientFlowContext():\")\n",
    "print(\"        out = model(x_param)\")\n",
    "print(\"    \")\n",
    "print(\"    loss = out.sum()\")\n",
    "print(\"    loss.backward()\")\n",
    "print(\"    opt.step()\")\n",
    "print()\n",
    "\n",
    "print(\"3️⃣ MEMORY-EFFICIENT VERSION for testing:\")\n",
    "print()\n",
    "\n",
    "# Test with smaller model if available\n",
    "try:\n",
    "    # Clear memory first\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create a minimal test\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(\"Creating memory-efficient test...\")\n",
    "    x_mini = torch.nn.Parameter(\n",
    "        torch.randn(1, 2, 2, 16, 16, device=device), \n",
    "        requires_grad=True\n",
    "    )\n",
    "    \n",
    "    # Simple test model\n",
    "    test_model = torch.nn.Sequential(\n",
    "        torch.nn.Conv3d(2, 4, 3, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv3d(4, 2, 3, padding=1)\n",
    "    ).to(device)\n",
    "    \n",
    "    for p in test_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # Test with and without detach\n",
    "    print(\"Test WITHOUT detach (should work):\")\n",
    "    out1 = test_model(x_mini)\n",
    "    loss1 = out1.sum()\n",
    "    loss1.backward()\n",
    "    grad_norm1 = x_mini.grad.norm().item() if x_mini.grad is not None else 0\n",
    "    print(f\"  Gradient norm: {grad_norm1:.6f}\")\n",
    "    \n",
    "    # Reset gradients\n",
    "    x_mini.grad = None\n",
    "    \n",
    "    print(\"Test WITH detach (should fail):\")\n",
    "    out2 = test_model(x_mini).detach()  # This breaks gradients\n",
    "    loss2 = out2.sum()\n",
    "    try:\n",
    "        loss2.backward()\n",
    "        grad_norm2 = x_mini.grad.norm().item() if x_mini.grad is not None else 0\n",
    "        print(f\"  Gradient norm: {grad_norm2:.6f}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"  ❌ Expected error: {str(e)[:50]}...\")\n",
    "    \n",
    "    print(\"✅ Demonstration complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Test error: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"4️⃣ FOR YOUR SPECIFIC CASE:\")\n",
    "print(\"   Your original cell #VSC-6bc925b5 should work if you:\")\n",
    "print(\"   a) Add the GradientFlowContext above\")\n",
    "print(\"   b) Wrap the model call: 'with GradientFlowContext(): out = new_model(x_param)'\")\n",
    "print(\"   c) Consider reducing input size or using gradient checkpointing for memory\")\n",
    "print()\n",
    "\n",
    "print(\"💡 The key insight: .detach() breaks gradients, so we temporarily disable it!\")\n",
    "print(\"   This is safe as long as you restore the original .detach() function.\")\n",
    "print()\n",
    "\n",
    "print(\"To use in your optimization:\")\n",
    "print(\"=\"*40)\n",
    "print(\"with GradientFlowContext():\")\n",
    "print(\"    out = new_model(x_param)\")\n",
    "print(\"loss = out.sum()\")\n",
    "print(\"loss.backward()  # Now this will work!\")\n",
    "print(\"opt.step()\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c94f474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1788271d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created new random `x_init`\n",
      "Start x norm: 3110.517090\n",
      "Running 5 optimization steps with gradient flow...\n",
      "❌ Error during optimization: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 93.10 GiB of which 1.69 GiB is free. Including non-PyTorch memory, this process has 91.39 GiB memory in use. Of the allocated memory 88.52 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "============================================================\n",
      "SUMMARY:\n",
      "- Added GradientFlowContext to temporarily disable .detach()\n",
      "- This allows gradients to flow through the model to the input x\n",
      "- Your original optimization should now work!\n",
      "============================================================\n",
      "❌ Error during optimization: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 93.10 GiB of which 1.69 GiB is free. Including non-PyTorch memory, this process has 91.39 GiB memory in use. Of the allocated memory 88.52 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "============================================================\n",
      "SUMMARY:\n",
      "- Added GradientFlowContext to temporarily disable .detach()\n",
      "- This allows gradients to flow through the model to the input x\n",
      "- Your original optimization should now work!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1440127/707026539.py\", line 47, in <module>\n",
      "    out = new_model(x_param)\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/glonet/src/glonet/modelp2.py\", line 82, in forward\n",
      "    predictions = self.mapsback(spatialtemporal_embed.to('cuda:0'), spatial_skip_feature.to('cuda:0')).contiguous()\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/glonet/src/glonet/utils.py\", line 115, in forward\n",
      "    Y = self.dec[-1](torch.cat([hid, enc1], dim=1))\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/glonet/src/glonet/utils.py\", line 41, in forward\n",
      "    y = self.conv(x)\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/glonet/src/glonet/utils.py\", line 26, in forward\n",
      "    y = self.conv(x)\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Odyssey/private/j25lee/miniforge3/envs/glon/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 93.10 GiB of which 1.69 GiB is free. Including non-PyTorch memory, this process has 91.39 GiB memory in use. Of the allocated memory 88.52 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "# DIRECT FIX: Modified version of your original cell #VSC-6bc925b5\n",
    "# This version should work with gradient flow\n",
    "\n",
    "# Context manager for enabling gradients\n",
    "class GradientFlowContext:\n",
    "    def __init__(self):\n",
    "        self.original_detach = torch.Tensor.detach\n",
    "    \n",
    "    def __enter__(self):\n",
    "        torch.Tensor.detach = lambda self: self\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        torch.Tensor.detach = self.original_detach\n",
    "\n",
    "# Your fixed optimization code\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "new_model.to(device)\n",
    "for p in new_model.parameters():\n",
    "    p.requires_grad = False\n",
    "new_model.eval()\n",
    "\n",
    "try:\n",
    "    x_init = x.detach().clone().to(device)\n",
    "    print('✓ Reusing existing `x` from earlier cells')\n",
    "except NameError:\n",
    "    x_init = torch.randn(1, 2, 5, 672, 1440, device=device)\n",
    "    print('✓ Created new random `x_init`')\n",
    "\n",
    "# Wrap the input as a Parameter so it can be optimized by an optimizer\n",
    "x_param = torch.nn.Parameter(x_init, requires_grad=True)\n",
    "opt = torch.optim.SGD([x_param], lr=1e-5, momentum=0.7)  # Smaller LR for stability\n",
    "\n",
    "start_norm = x_param.data.norm().item()\n",
    "print(f'Start x norm: {start_norm:.6f}')\n",
    "\n",
    "# Fixed training loop with gradient flow\n",
    "steps = 5  # Reduced for memory constraints\n",
    "print(f\"Running {steps} optimization steps with gradient flow...\")\n",
    "clear_gpu_memory()\n",
    "try:\n",
    "    for i in range(steps):\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # 🔥 KEY FIX: Use context manager to enable gradients\n",
    "        with GradientFlowContext():\n",
    "            out = new_model(x_param)\n",
    "\n",
    "        # Helper to find first tensor in output\n",
    "        def first_tensor(o):\n",
    "            if torch.is_tensor(o):\n",
    "                return o\n",
    "            if isinstance(o, (list, tuple)):\n",
    "                for v in o:\n",
    "                    t = first_tensor(v)\n",
    "                    if t is not None:\n",
    "                        return t\n",
    "            if isinstance(o, dict):\n",
    "                for v in o.values():\n",
    "                    t = first_tensor(v)\n",
    "                    if t is not None:\n",
    "                        return t\n",
    "            return None\n",
    "\n",
    "        out_t = first_tensor(out)\n",
    "        if out_t is None:\n",
    "            print('❌ Model returned no tensor; cannot compute loss. Stopping.')\n",
    "            break\n",
    "\n",
    "        # Check if gradients are preserved\n",
    "        print(f\"Step {i}: out_t.requires_grad={out_t.requires_grad}, has_grad_fn={out_t.grad_fn is not None}\")\n",
    "        \n",
    "        if not out_t.requires_grad:\n",
    "            print(f\"❌ Step {i}: Output tensor doesn't require gradients\")\n",
    "            break\n",
    "\n",
    "        loss = out_t.sum()  # Simple scalar to exercise gradients\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = x_param.grad.norm().item() if x_param.grad is not None else float('nan')\n",
    "        print(f'✅ Step {i:02d} loss={loss.item():.6e} x.grad_norm={grad_norm:.6e} x.norm={x_param.data.norm().item():.6f}')\n",
    "\n",
    "        if grad_norm == 0 or grad_norm != grad_norm:  # Check for zero or NaN gradients\n",
    "            print(f\"⚠️  Warning: Gradient norm is {grad_norm}\")\n",
    "\n",
    "        opt.step()\n",
    "        \n",
    "        # Memory management\n",
    "        del out, out_t, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    end_norm = x_param.data.norm().item()\n",
    "    print(f'End x norm: {end_norm:.6f} (changed by {end_norm - start_norm:.6f})')\n",
    "    print('final x_param.requires_grad:', x_param.requires_grad)\n",
    "    \n",
    "    # Final verification\n",
    "    with GradientFlowContext():\n",
    "        final_out = new_model(x_param)\n",
    "    final_out_t = first_tensor(final_out)\n",
    "    \n",
    "    print('final_out_t.requires_grad:', getattr(final_out_t, 'requires_grad', None))\n",
    "    print('final_out_t.grad_fn:', getattr(final_out_t, 'grad_fn', None))\n",
    "    \n",
    "    if getattr(final_out_t, 'requires_grad', False):\n",
    "        print('🎉 SUCCESS! Gradients now flow from model output to initial condition x')\n",
    "    else:\n",
    "        print('❌ Still having gradient flow issues')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during optimization: {e}\")\n",
    "    # Memory cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY:\")\n",
    "print(\"- Added GradientFlowContext to temporarily disable .detach()\")\n",
    "print(\"- This allows gradients to flow through the model to the input x\")\n",
    "print(\"- Your original optimization should now work!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c382bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
